{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration de Cuda dans Google Colab"
      ],
      "metadata": {
        "id": "qE-HbkLWqjJK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJjU5zI_-tjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86de07bd-899c-413c-e85d-55c5ac75b375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc -V"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "id": "HBZjh4P4-1HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c36d14a-e283-4f64-8df9-42385f458d44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-dwfvsthc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-dwfvsthc\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4305 sha256=84ca9f2cbf958736c254af699e4d82e4f130eca69c35e1fe105d1cd0632f6d9f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4wps55m3/wheels/db/c1/1f/a2bb07bbb4a1ce3c43921252aeafaa6205f08637e292496f04\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On vérifie que l'on est bien connecté au GPU"
      ],
      "metadata": {
        "id": "YJw3IWdqtpfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "9gl_4Pn7_JR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce797034-9761-4bbb-fc33-e2f4dc24c833"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 19 17:44:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    30W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du plugin nvcc permettant de compiler/executer les programmes Cuda"
      ],
      "metadata": {
        "id": "NlBBvVVOt4Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "id": "6YGSePh_Q_DP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa99dad5-7f54-469c-a5d4-8f4dd3122874"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Un makefile est déjà à votre disposition pour compiler les programme du TP\n"
      ],
      "metadata": {
        "id": "0zrHyBuVuQpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Executez la cellule du Makefile\n",
        "\n",
        "Le makefile a été modifié pour les programmes puisse s'exécuter avec la GPU premium.\n",
        "\n"
      ],
      "metadata": {
        "id": "AWS3uSGwu2o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Makefile\n",
        "# Change the example variable to build a different source module (e.g. EXAMPLE=exercice01)\n",
        "EXAMPLE=program\n",
        "\n",
        "# Makefile variables \n",
        "# Add extra targets to OBJ with space separator e.g. If there is as source file random.c then add random.o to OBJ)\n",
        "# Add any additional dependancies (header files) to DEPS. e.g. if there is aheader file random.h required by your source modules then add this to DEPS.\n",
        "CC=gcc\n",
        "CFLAGS= -O3 -Wextra -fopenmp\n",
        "NVCC=nvcc\n",
        "NVCC_FLAGS= -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87\n",
        "OBJ=$(EXAMPLE).o\n",
        "DEPS=\n",
        "\n",
        "# Build rule for object files ($@ is left hand side of rule, $< is first item from the right hand side of rule)\n",
        "%.o : %.cu $(DEPS)\n",
        "\t$(NVCC) -c -o $@ $< $(NVCC_FLAGS) $(addprefix -Xcompiler ,$(CCFLAGS))\n",
        "\n",
        "# Make example ($^ is all items from right hand side of the rule)\n",
        "$(EXAMPLE) : $(OBJ)\n",
        "\t$(NVCC) -o $@ $^ $(NVCC_FLAGS) $(addprefix -Xcompiler ,$(CCFLAGS))\n",
        "\n",
        "# PHONY prevents make from doing something with a filename called clean\n",
        ".PHONY : clean\n",
        "clean:\n",
        "\trm -rf $(EXAMPLE) $(OBJ)"
      ],
      "metadata": {
        "id": "2VMs5wjdRU0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9748b34d-70ef-4e23-dd89-730b4995d53e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP3\n"
      ],
      "metadata": {
        "id": "ri3OU-i0vt2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonctions utilitaires\n",
        "\n",
        "utils.h est un header contenant des fonctions utilitaires qui seront utilisés par nos programmes"
      ],
      "metadata": {
        "id": "aimw4eG5H5KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.h\n",
        "#ifndef __UTILS_H__\n",
        "#define __UTILS_H__\n",
        "#include <stdio.h>\n",
        "\n",
        "static void HandleError( cudaError_t err,\n",
        "                         const char *file,\n",
        "                         int line ) {\n",
        "    if (err != cudaSuccess) {\n",
        "        printf( \"%s in %s at line %d\\n\", cudaGetErrorString( err ),\n",
        "                file, line );\n",
        "        exit( EXIT_FAILURE );\n",
        "    }\n",
        "}\n",
        "#define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ ))\n",
        "\n",
        "\n",
        "#define HANDLE_NULL( a ) {if (a == NULL) { \\\n",
        "                            printf( \"Host memory failed in %s at line %d\\n\", \\\n",
        "                                    __FILE__, __LINE__ ); \\\n",
        "                            exit( EXIT_FAILURE );}}\n",
        "\n",
        "template< typename T >\n",
        "void swap( T& a, T& b ) {\n",
        "    T t = a;\n",
        "    a = b;\n",
        "    b = t;\n",
        "}\n",
        "\n",
        "struct event_pair\n",
        "{\n",
        "  cudaEvent_t start;\n",
        "  cudaEvent_t end;\n",
        "};\n",
        "\n",
        "inline void start_timer(event_pair * p)\n",
        "{\n",
        "  cudaEventCreate(&p->start);\n",
        "  cudaEventCreate(&p->end);\n",
        "  cudaEventRecord(p->start, 0);\n",
        "}\n",
        "\n",
        "\n",
        "inline void stop_timer(event_pair * p, char * kernel_name)\n",
        "{\n",
        "  cudaEventRecord(p->end, 0);\n",
        "  cudaEventSynchronize(p->end);\n",
        "  \n",
        "  float elapsed_time;\n",
        "  cudaEventElapsedTime(&elapsed_time, p->start, p->end);\n",
        "  printf(\"%s took %.4f ms\\n\",kernel_name, elapsed_time);\n",
        "  cudaEventDestroy(p->start);\n",
        "  cudaEventDestroy(p->end);\n",
        "}\n",
        "\n",
        "void* big_random_block( int size ) {\n",
        "    unsigned char *data = (unsigned char*)malloc( size );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "int* big_random_block_int( int size ) {\n",
        "    int *data = (int*)malloc( size * sizeof(int) );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "\n",
        "// a place for common kernels - starts here\n",
        "\n",
        "__device__ unsigned char value( float n1, float n2, int hue ) {\n",
        "    if (hue > 360)      hue -= 360;\n",
        "    else if (hue < 0)   hue += 360;\n",
        "\n",
        "    if (hue < 60)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*hue/60));\n",
        "    if (hue < 180)\n",
        "        return (unsigned char)(255 * n2);\n",
        "    if (hue < 240)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*(240-hue)/60));\n",
        "    return (unsigned char)(255 * n1);\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( unsigned char *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset*4 + 0] = value( m1, m2, h+120 );\n",
        "    optr[offset*4 + 1] = value( m1, m2, h );\n",
        "    optr[offset*4 + 2] = value( m1, m2, h -120 );\n",
        "    optr[offset*4 + 3] = 255;\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( uchar4 *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset].x = value( m1, m2, h+120 );\n",
        "    optr[offset].y = value( m1, m2, h );\n",
        "    optr[offset].z = value( m1, m2, h -120 );\n",
        "    optr[offset].w = 255;\n",
        "}\n",
        "\n",
        "\n",
        "#if _WIN32\n",
        "    //Windows threads.\n",
        "    #include <windows.h>\n",
        "\n",
        "    typedef HANDLE CUTThread;\n",
        "    typedef unsigned (WINAPI *CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC unsigned WINAPI\n",
        "    #define  CUT_THREADEND return 0\n",
        "\n",
        "#else\n",
        "    //POSIX threads.\n",
        "    #include <pthread.h>\n",
        "\n",
        "    typedef pthread_t CUTThread;\n",
        "    typedef void *(*CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC void\n",
        "    #define  CUT_THREADEND\n",
        "#endif\n",
        "\n",
        "//Create thread.\n",
        "CUTThread start_thread( CUT_THREADROUTINE, void *data );\n",
        "\n",
        "//Wait for thread to finish.\n",
        "void end_thread( CUTThread thread );\n",
        "\n",
        "//Destroy thread.\n",
        "void destroy_thread( CUTThread thread );\n",
        "\n",
        "//Wait for multiple threads.\n",
        "void wait_for_threads( const CUTThread *threads, int num );\n",
        "\n",
        "#if _WIN32\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void *data){\n",
        "        return CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE)func, data, 0, NULL);\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        WaitForSingleObject(thread, INFINITE);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        TerminateThread(thread, 0);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        WaitForMultipleObjects(num, threads, true, INFINITE);\n",
        "\n",
        "        for(int i = 0; i < num; i++)\n",
        "            CloseHandle(threads[i]);\n",
        "    }\n",
        "\n",
        "#else\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void * data){\n",
        "        pthread_t thread;\n",
        "        pthread_create(&thread, NULL, func, data);\n",
        "        return thread;\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        pthread_join(thread, NULL);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        pthread_cancel(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        for(int i = 0; i < num; i++)\n",
        "            end_thread( threads[i] );\n",
        "    }\n",
        "\n",
        "#endif\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#endif  // __UTILS_H__"
      ],
      "metadata": {
        "id": "78ooPnc8H3wB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d229f1-f591-4ded-fa87-e4599bbb690f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Convolution simple (1D)\n",
        "\n",
        "La convolution est une opération permettant d'alterer une image en faisant glisser un filtre sur cette image. Cela peut permettre d'ajouter du flou sur l'image, de detecter le contours de certains objets d'un image et bien d'autres.\n",
        "(Cf images https://github.com/GPUModule/cuda-TP3/blob/main/README.md)\n",
        "\n",
        "Le but de cet exercice est de programmer une convolution simple, ou l'on fait glisser un filtre de taille 3 (tableau de taille 3) sur un vecteur a de taille N\n",
        "\n",
        "Le vecteur c contenant le résultat de la convolution n'a pas la même taille que le que vecteur a d'origine Pour trouver la taille du tableau résultant, il suffit de faire **n_c = N - f + 1**.\n",
        "\n",
        "Une fonction **random_floats** vous est fournit permettant d'initialiser un tableau de taille N.\n",
        "\n",
        "### Complétez la fonction main.\n",
        "#### 1.1 Allouez la mémoire hôte à l'aide de malloc\n",
        "#### 1.2 Allouez la mémoire GPU à l'aide de cudaMalloc\n",
        "#### 1.3 Copiez les vecteurs a et b dans la mémoire GPU (respectivement d_a et d_b) \n",
        "#### 1.4 Copiez la mémoire du de d_c dans la mémoire hôte (c)\n",
        "#### 1.5 LIbéré la mémoire.\n",
        "\n",
        "\n",
        "### Convolution simple\n",
        "#### 1.6 Completez le kernel simple_convolution_1D_kernel\n",
        "- Créez une variable temporaire stockant le résultat de la convolution pour chaque thread.\n",
        "- Chaque thread multipliera chaque élément de f avec les éléments correspondant de a, et sommera le résultat de chaque multiplication. La résultat de la somme sera stocké dans c.\n",
        "\n",
        "#### 1.7 Executez le programme \n",
        "Le programme doit afficher un tableau a de 6 éléments, un filtre filter de 3 éléments et le résultat c de 4 éléments.\n",
        "\n",
        "On commence avec un tableau de taille 6 afin de valider rapidement le résultat de la convolution.\n",
        "\n",
        "Une fois le résultat validé, changez les paramètres suivant :\n",
        "```\n",
        "\tdim3 blocksPerGrid(1) --> dim3 blocksPerGrid((N + THREADS_PER_BLOCK -1)/THREADS_PER_BLOCK); \n",
        "```\n",
        "\n",
        "```\n",
        "  #define N 6 --> #define N 2048\n",
        "  #define THREADS_PER_BLOCK 6 --> #define THREADS_PER_BLOCK 256\n",
        "```\n",
        "  \n",
        "Commentez l'affichage des vecteurs:\n",
        "```\n",
        "print_array(a, N, \"a\"); --> print_array(a, N, \"a\"); \n",
        "print_array(filter, f, \"filter\"); --> print_array(filter, f, \"filter\");\n",
        "\n",
        "print_array(c, n_c, \"c\"); --> print_array(c, n_c, \"c\");\n",
        "```\n",
        "### Convolution simple dans la mémoire partagée.\n",
        "\n",
        "#### 1.8 Dans la fonction main, écrivez les instructions nécessaire à la création d'un vecteur cs et d'un vecteur d_cs de même dimension. cs récupérera le résultat de la convolution faite en utilisant la mémoire partagée, stocké dans d_cs.\n",
        "\n",
        "#### 1.9 Complétez le kernel shared_convolution_1D\n",
        "- Creer un vecteur s_data en mémoire partagée d'une taille égale à la dimension d'un bloc.\n",
        "- Initialisez s_data avec les éléments du vecteur a. Utilisez l'index global des threads pour récupérer les bon éléments du vecteur a.\n",
        "- Synchroniser les threads (\\_\\_syncthreads()) pour qu'ils attendent que s_data soit bien initialisé. \n",
        "\n",
        "Modifié le code afin de pouvoir utiliser cette mémoire partagée. \n",
        "\n",
        "### 1.10 Decommentez les instructions suivante :\n",
        "\n",
        "```\n",
        "//errors = validate(c, cs, n_c);\n",
        "//printf(\"CUDA GPU result has %d errors.\\n\", errors);\n",
        "```\n",
        "\n",
        "Cela de s'assurer que le résultat de la convolution simple utilisant la mémoire globale et de la convolution simple utilisant la mémoire partagée sont identiques.\n",
        "\n",
        "Faites attentions aux limites du tableau s_data, chaque block creer un tableau s_data de taille le nombre de threads de ce block. il récupère la sous-partie correspondante du tableau a dans s_data et réalise ensuite la convolution. Les threads d'un block n'ont pas accès au tableau s_data d'autres blocks.\n"
      ],
      "metadata": {
        "id": "uB1BFcg5Qxb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile program.cu\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"utils.h\"\n",
        "\n",
        "\n",
        "#define N 2048\n",
        "#define THREADS_PER_BLOCK 256\n",
        "#define SQRT_THREADS_PER_BLOCK sqrt(THREADS_PER_BLOCK)\n",
        "\n",
        "void random_floats(float *a, int n);\n",
        "void print_array(float *a, int n, char *name);\n",
        "int validate(float *a, float *ref, int n);\n",
        "\n",
        "__global__ void simple_convolution1D_kernel(float* c, float* a, float* filter, int f, int n) {// N = 6 -> 0.03 ms. N = \n",
        "\t\tint row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\t\tif(row < n - f + 1){\n",
        "\t\t\tfloat Cvalue = 0;\n",
        "\t\t\tfor(int i = 0; i < f; i++){\n",
        "\t\t\t\t\t\tCvalue += a[row + i] * filter[i];\n",
        "\t\t\t}\n",
        "\t\t\tc[row] = Cvalue;\n",
        "\t\t}\n",
        "\t\t\n",
        "}\n",
        "\n",
        "__global__ void shared_convolution1D_kernel(float* c, float* a, float* filter, int f, int n) {\n",
        "\t\t\n",
        "\t\t__shared__ float s_data[THREADS_PER_BLOCK + 2]; // +2 pour les variables qui ne seront pas accessible\n",
        "\t\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\tint s_data_idx = threadIdx.x +1;\n",
        "\t\t\n",
        "\t\ts_data[s_data_idx] = (i < n) ? a[i] : 0.0f;\n",
        "\t\ts_data[blockDim.x + 1] = (blockIdx.x < N - 1) ? a[blockDim.x * blockIdx.x + blockDim.x] : 0.0f;\n",
        "\t\ts_data[blockDim.x + 2] = (blockIdx.x < N - 1) ? a[blockDim.x * blockIdx.x + blockDim.x + 1] : 0.0f;\n",
        "\t\ts_data[0] = (blockIdx.x > 0) ?  a[i - 1] : 0.0f;\t\t\n",
        "\n",
        "\t\t__syncthreads();\n",
        "\n",
        "\t\tfloat sum = 0.0f;\n",
        "\t\tfor (int j = 0; j < f; j++) {\n",
        "\t\t\tsum += filter[j] * s_data[s_data_idx + j];\n",
        "\t\t}\n",
        "\t\tc[i] = sum;\n",
        "}\n",
        "\n",
        "\n",
        "int main(void) {\n",
        "\tsrand( time( NULL ) );\n",
        "\n",
        "\tfloat *a, *filter, *c, *cs;\n",
        "\tfloat *d_a, *d_filter, *d_c, *d_cs;\n",
        "\tint errors;\n",
        "\t\n",
        "\tint f = 3;\n",
        "\tint n_c = N - f + 1 ;\n",
        "\n",
        "\tevent_pair timer;\n",
        "\n",
        "  // 1.1. Allocation de la mémoire hôte\n",
        "\tunsigned int filter_size = f * sizeof(float);\n",
        "\tunsigned int size = N * sizeof(float);\n",
        "\tunsigned int c_size = n_c * sizeof(float);\n",
        "\ta = (float*) malloc(size);\n",
        "\tfilter = (float*) malloc(filter_size);\n",
        "\tc = (float*) malloc(c_size);\n",
        "\tcs = (float*) malloc(c_size);\n",
        "\n",
        "\t// 1.2. Allocation de la mémoire GPU\n",
        "\t// A completer\n",
        "\tHANDLE_ERROR( cudaMalloc(&d_a,  size ) );\n",
        "  HANDLE_ERROR( cudaMalloc(&d_filter, filter_size ) );\n",
        "  HANDLE_ERROR( cudaMalloc(&d_c, c_size ) );\n",
        "\tHANDLE_ERROR( cudaMalloc(&d_cs, c_size) );\n",
        "\n",
        "\trandom_floats(a, N);\n",
        "\trandom_floats(filter, f);\n",
        "\t//print_array(a, N, \"a\");\n",
        "\t//print_array(filter, f, \"filter\");\n",
        "\n",
        "\t// 1.3. Copie de la mémoire Hôte vers la mémoire GPU\n",
        "\t// A completer\n",
        "\tHANDLE_ERROR( cudaMemcpy( d_a, a, size, cudaMemcpyHostToDevice ) );\n",
        "  HANDLE_ERROR( cudaMemcpy( d_filter, filter, filter_size, cudaMemcpyHostToDevice) ); \n",
        "\n",
        "\t// Lancement du kernel.\n",
        "\tdim3 blocksPerGrid((N + THREADS_PER_BLOCK -1)/THREADS_PER_BLOCK);\n",
        "\tdim3 threadsPerBlock(THREADS_PER_BLOCK);\n",
        "\t//start_timer(&timer);\n",
        "\tsimple_convolution1D_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_c, d_a, d_filter, f, N);\n",
        "\t//stop_timer(&timer,\"Convolution 1D sur GPU\");\n",
        "\t\n",
        "\t// 1.4. Copie de la mémoire GPU --> CPU\n",
        "\tHANDLE_ERROR(cudaMemcpy(c, d_c ,c_size, cudaMemcpyDeviceToHost) );\n",
        "\n",
        "\n",
        "\t//print_array(c, n_c, \"c\");\n",
        "\t\n",
        "\tstart_timer(&timer);\n",
        "\tshared_convolution1D_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_cs, d_a, d_filter, f, N);\n",
        "\tstop_timer(&timer,\"Convolution 1D shared sur GPU\");\n",
        "\t\n",
        "\n",
        "\tHANDLE_ERROR(cudaMemcpy(cs, d_cs ,c_size, cudaMemcpyDeviceToHost));\t\n",
        "\n",
        "\t// validate\n",
        "\terrors = validate(c, cs, n_c);\n",
        "\tprintf(\"CUDA GPU result has %d errors.\\n\", errors);\n",
        "\n",
        "\t// 1.5. Libérer la mémoire.\n",
        "\n",
        "\tfree(a);\n",
        "\tfree(filter);\n",
        "\tfree(c);\n",
        "\tcudaFree(d_c);\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_filter);\n",
        "  \n",
        "\treturn 0;\n",
        "}\n",
        "\n",
        "void random_floats(float *a, int n)\n",
        "{\n",
        "\tfor (unsigned int i = 0; i < n; i++){\n",
        "\t\t\ta[i] = (float)(rand() % 101);\n",
        "\t}\n",
        "}\n",
        "\n",
        "void print_array(float *a, int n, char*name){\n",
        "\n",
        "\tprintf(\"%s : [ \",name);\n",
        "\tfor (unsigned int i = 0; i < n; i++){\n",
        "\t\t\tprintf(\"%.4f \",a[i]);\n",
        "\t}\n",
        "\tprintf(\"]\\n\");\n",
        "}\n",
        "\n",
        "int validate(float *a, float *ref, int n){\n",
        "\tint errors = 0;\n",
        "\tfor (unsigned int i = 0; i < n; i++){\n",
        "\t\tif (a[i] != ref[i]){\n",
        "\t\t\terrors++;\n",
        "\t\t\tfprintf(stderr, \"ERROR at index %d: GPU result %f does not match CPU value of %f\\n\", i, a[i], ref[i]);\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\treturn errors;\n",
        "}"
      ],
      "metadata": {
        "id": "Rm-7gDO8BGH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856aa428-6e64-43bc-f0ec-97b745ce57fc"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting program.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "id": "PfakPQ1CHLUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7cf422-9fa7-465e-9193-3d3cac8aca25"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc -c -o program.o program.cu -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87 \n",
            "\u001b[01m\u001b[0m\u001b[01mprogram.cu(103)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mprogram.cu(103)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mprogram.cu(103)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mprogram.cu(103)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mprogram.cu(103)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "nvcc -o program program.o -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./program"
      ],
      "metadata": {
        "id": "F4BRhsCtHLo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74aa02c1-4c2f-4396-c23a-5096a02b0ecd"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolution 1D shared sur GPU took 0.0105 ms\n",
            "CUDA GPU result has 0 errors.\n"
          ]
        }
      ]
    }
  ]
}